{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b134f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdffe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.makedirs(r\"D:\\hf_cache\\hub\", exist_ok=True)\n",
    "os.makedirs(r\"~/luudh/MyFile/vr_lab/hf_cache/hub\", exist_ok=True)\n",
    "\n",
    "# Best: point directly to the hub cache folder\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = r\"~/luudh/MyFile/vr_lab/hf_cache/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9653e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luudh/.conda/envs/my_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d316f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "fine_tuned_model = os.path.expanduser(\n",
    "    \"~/luudh/MyFile/AI_Scheduling/Llama-3.1-8B-Instruct-finetuned-version2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f1c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e43007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_model(base, fine_tuned):\n",
    "    #reload tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #print(tokenizer.chat_template)\n",
    "\n",
    "    # QLoRA config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    max_memory = {0: \"46GiB\", \"cpu\": \"64GiB\"} # adjust according to your GPU\n",
    "\n",
    "    \"\"\"\n",
    "    max_memory = {\n",
    "        0: \"6GiB\",         \n",
    "        \"cpu\": \"32GiB\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",                  # let it split GPU/CPU\n",
    "        attn_implementation=\"eager\",        # avoid flash-attn\n",
    "        torch_dtype=torch.float16,\n",
    "        max_memory=max_memory,\n",
    "    )\n",
    "\n",
    "    tokenizer.chat_template = None\n",
    "    base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "\n",
    "    #merge adapter with base model\n",
    "    model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "\n",
    "    model = model.merge_and_unload()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f18359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/home/luudh/.conda/envs/my_env/lib/python3.13/site-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'target_parameters', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "/home/luudh/.conda/envs/my_env/lib/python3.13/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid JSON found.\n",
      "--- RAW ---\n",
      " system\n",
      "You are an assistant that extracts structured metadata from task descriptions.\n",
      "\n",
      "Return the following fields:\n",
      "- Task Priority (high, medium, low)\n",
      "- Task Complexity (1-10 scale)\n",
      "- Required Skills (list of skills, including both technical and soft/inferred skills)\n",
      "- Estimated Time (in days), calculated as the number of days between today and the task deadline.\n",
      "  The Estimated Time must be less than or equal to the total number of days until the Deadline.\n",
      "- Deadline (in YYYY-MM-DD HH:MM format)\n",
      "- NTS_skills (object with the keys: ambiguity_tolerance, communication, planning, collaboration, reasoning, risk_awareness, ownership, stakeholder_mgmt; each value is an integer 1-5)\n",
      "- importance (object with the keys: ambiguity_tolerance, communication, planning, collaboration, reasoning, risk_awareness, ownership, stakeholder_mgmt; each value is an integer 1-5)\n",
      "\n",
      "Respond only with the structured output in JSON format.\n",
      "\n",
      "user\n",
      "Design and implement a machine learning pipeline to predict customer churn using historical transaction and interaction data. \n",
      "Use Python with scikit-learn or XGBoost, and ensure the model can be evaluated using ROC-AUC and F1 metrics. \n",
      "Collaborate with the data engineering team to source clean datasets and set up daily retraining jobs via Airflow. \n",
      "The project must be delivered by 2026-06-26 14:00.\n",
      "assistant\n",
      "{\n",
      "  \"Task\": {\n",
      "  \"Task Priority\": \"high\",\n",
      "  \"Task Complexity\": 8,\n",
      "  \"Required Skills\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Python\",\n",
      "    \"Scikit-learn\",\n",
      "    \"XGBoost\",\n",
      "    \"Data Engineering\",\n",
      "    \"Airflow\",\n",
      "    \"Collaboration\"\n",
      "  ],\n",
      "  \"Estimated Time\": 90,\n",
      "  \"Deadline\": \"2026-06-26 14:00\",\n",
      "  \"NTS_skills\": {\n",
      "    \"ambiguity_tolerance\": 5,\n",
      "    \"communication\": 5,\n",
      "    \"planning\": 5,\n",
      "    \"collaboration\": 5,\n",
      "    \"reasoning\": 5,\n",
      "    \"risk_awareness\": 5,\n",
      "    \"ownership\": 5,\n",
      "    \"stakeholder_mgmt\": 5\n",
      "  },\n",
      "  \"importance\": {\n",
      "    \"ambiguity_tolerance\": 5,\n",
      "    \"communication\": 5,\n",
      "    \"planning\": 5,\n",
      "    \"collaboration\": 5,\n",
      "    \"reasoning\": 5,\n",
      "    \"risk_awareness\": 5,\n",
      "    \"ownership\": 5,\n",
      "    \"stakeholder_mgmt\": 5\n",
      "  }\n",
      "}assistant\n",
      "{\n",
      "  \"Task Priority\": \"high\",\n",
      "  \"Task Complexity\": 8,\n",
      "  \"Required Skills\": [\n",
      "    \"Machine Learning\",\n",
      "    \"Python\",\n",
      "    \"Scikit-learn\",\n",
      "    \"XGBoost\",\n",
      "    \"Data\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "\n",
    "def extract_json(text: str):\n",
    "    # 1) take everything from first { to last }\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return None\n",
    "\n",
    "    snippet = text[start:end+1].strip()\n",
    "\n",
    "    # 2) remove code fences if present\n",
    "    if snippet.startswith(\"```\"):\n",
    "        # remove ```json or ``` and trailing ```\n",
    "        snippet = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", snippet)\n",
    "        snippet = re.sub(r\"```$\", \"\", snippet).strip()\n",
    "\n",
    "    # 3) fix python-style literals if model produced them\n",
    "    snippet = (\n",
    "        snippet.replace(\"True\", \"true\")\n",
    "               .replace(\"False\", \"false\")\n",
    "               .replace(\"None\", \"null\")\n",
    "    )\n",
    "\n",
    "    # 4) now try to load\n",
    "    try:\n",
    "        return json.loads(snippet)\n",
    "    except json.JSONDecodeError:\n",
    "        # optional: print(snippet) to inspect\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate(user_input, base_model=base_model, fine_tuned_model=fine_tuned_model):\n",
    "    model, tokenizer = start_model(base_model, fine_tuned_model)\n",
    "\n",
    "    instruction = \"\"\"You are an assistant that extracts structured metadata from task descriptions.\n",
    "\n",
    "Return the following fields:\n",
    "- Task Priority (high, medium, low)\n",
    "- Task Complexity (1-10 scale)\n",
    "- Required Skills (list of skills, including both technical and soft/inferred skills)\n",
    "- Estimated Time (in days), calculated as the number of days between today and the task deadline.\n",
    "  The Estimated Time must be less than or equal to the total number of days until the Deadline.\n",
    "- Deadline (in YYYY-MM-DD HH:MM format)\n",
    "- NTS_skills (object with the keys: ambiguity_tolerance, communication, planning, collaboration, reasoning, risk_awareness, ownership, stakeholder_mgmt; each value is an integer 1-5)\n",
    "- importance (object with the keys: ambiguity_tolerance, communication, planning, collaboration, reasoning, risk_awareness, ownership, stakeholder_mgmt; each value is an integer 1-5)\n",
    "\n",
    "Respond only with the structured output in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    data = extract_json(text)\n",
    "    if data is not None:\n",
    "        print(data)\n",
    "        return data\n",
    "    else:\n",
    "        # fall back: show the raw text so you can see what model produced\n",
    "        print(\"No valid JSON found.\\n--- RAW ---\\n\", text)\n",
    "        return None\n",
    "\n",
    "user_input = \"\"\"Design and implement a machine learning pipeline to predict customer churn using historical transaction and interaction data. \n",
    "Use Python with scikit-learn or XGBoost, and ensure the model can be evaluated using ROC-AUC and F1 metrics. \n",
    "Collaborate with the data engineering team to source clean datasets and set up daily retraining jobs via Airflow. \n",
    "The project must be delivered by 2026-06-26 14:00.\"\"\"\n",
    "\n",
    "generate(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
